<!DOCTYPE HTML>
<html>
	<head>
		<title>Fifth AAAI Workshop on Privacy-Preserving Artificial Intelligence</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta name="keywords" content="PPAI,PPAI24,PPAI2024,PPAI 24,PPAI 2024,AAAI,AAAI24,AAAI 2024,Privacy Preserving AI,Privacy Preserving Artificial Intelligence,AI, Differential Privacy,Privacy,Preserving,Artificial,Intelligence"/>
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">
				<!-- Main -->
				<div id="main">
					<div class="inner">
						<!-- Banner -->
						<section id="banner">
							<div class="content">
								<header>
									<h1>PPAI-24: The 5th AAAI Workshop on Privacy-Preserving Artificial Intelligence</h1>
									<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-calendar3" viewBox="0 0 16 16">
									  <path d="M14 0H2a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V2a2 2 0 0 0-2-2zM1 3.857C1 3.384 1.448 3 2 3h12c.552 0 1 .384 1 .857v10.286c0 .473-.448.857-1 .857H2c-.552 0-1-.384-1-.857V3.857z"/>
									  <path d="M6.5 7a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm-9 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm-9 3a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2zm3 0a1 1 0 1 0 0-2 1 1 0 0 0 0 2z"/>
									</svg>
									Monday, February 26, 2024
 									<br>
                                		<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="mr-1 bi bi-geo-alt" viewBox="0 0 16 16">
                                  		<path d="M12.166 8.94c-.524 1.062-1.234 2.12-1.96 3.07A31.493 31.493 0 0 1 8 14.58a31.481 31.481 0 0 1-2.206-2.57c-.726-.95-1.436-2.008-1.96-3.07C3.304 7.867 3 6.862 3 6a5 5 0 0 1 10 0c0 .862-.305 1.867-.834 2.94zM8 16s6-5.686 6-10A6 6 0 0 0 2 6c0 4.314 6 10 6 10z"/>
                                  		<path d="M8 8a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm0 1a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/>
                                		</svg>
                                		<strong>PPAI is an in-person event at</strong>:  Vancouver Canada,
                                		Vancouver Convention Center - West Building (ROOM 217)
                                		<br>
                                		PPAI will also be live-streamed at <a href="https://underline.io/events/443/sessions?searchGroup=lecture&eventSessionId=16617">underline.io</a>
								</header>
							</div>
							<span class="center">
								<img class="center" src="images/ppai-logo3.png" height="400" alt=""/>
						</section>

						<!-- Section -->
						<section>
							<header class="major" id="scope">
								<h2>Scope and Topics</h2>
							</header>
							<p>
								The rise of machine learning, optimization, and Large Language Models (LLMs) has created new paradigms for computing, but it has also ushered in complex privacy challenges. The intersection of AI and privacy is not merely a technical dilemma but a societal concern that demands careful considerations.
								<br>
								The Privacy Preserving AI workshop, in its 5th edition, will provide a
								multi-disciplinary platform for researchers, AI practitioners, and
								policymakers to focus on the theoretical and practical aspects of
								designing privacy-preserving AI systems and algorithms. The emphasis
								will be placed on <strong>policy considerations, broader implications
									of privacy in LLMs, and the societal impact of privacy within AI</strong>. 
								</p>

							<h3>Topics</h3>
							We invite three categories of contributions: technical (research) papers, position papers, and systems descriptions on these subjects:
							<ul>
							<li>Differential privacy applications</li>
							<li>Privacy and Fairness interplay</li>
							<li>Legal frameworks and privacy policies</li>
							<li>Privacy-centric machine learning and optimization</li>
							<li>Benchmarking: test cases and standards</li>
							<li>Ethical considerations of LLMs on users' privacy</li>
							<li>The impact of LLMs on personal privacy in various applications like chatbots, recommendation systems, etc.</li>
							<li>Case studies on real-world privacy challenges and solutions in deploying LLMs</li>
							<li>Privacy-aware evaluation metrics and benchmarks specifically for LLMs</li>
							<li>Interdisciplinary perspectives on AI applications, including sociological and economic views on privacy</li>
							<li>Evaluating models to audit and/or minimize data leakages</li>
							<li>Privacy and Fairness</li>
							<li>Privacy and causality </li>
							<li>Privacy-preserving optimization and machine learning</li>
						</ul>
							<p>
								Finally, the workshop will welcome papers that describe the release of privacy-preserving benchmarks and data sets that can be used by the community to solve fundamental problems of interest, including in machine learning and optimization for health systems and urban networks, to mention but a few examples.
							</p>

							<h3>Format</h3>

							<p>
								The workshop will be a one-day  meeting.
								The workshop will include a number of technical  sessions, a
								poster session where presenters can discuss their work, with
								the aim of  further fostering collaborations, multiple invited
								speakers covering crucial challenges for  the field of
								privacy-preserving AI applications, including policy and
								societal impacts, a number of tutorial talks, and will
								conclude with a panel discussion.
	 						</p>

						<br>

						<header class="major" id="dates">
							<h2>Important Dates</h2>
						</header>
							<ul>
							<li><strong>November 22, 2023</strong> – Submission Deadline <font color="#f56a6a">[Extended]</font></li>
							<li><strong>December 12, 2023</strong>  – NeurIPS/AAAI Fast Track Submission Deadline</li>
							<li><strong>December 22, 2023</strong> – Acceptance Notification</li>
							<li><strong>January 15, 2024</strong> – Student Scholarship Program Deadline <font color="#f56a6a">[Extended]</font></li>
							<li><strong>February 26, 2024</strong> – Workshop Date </li>
							</ul>
						<br>

							<header class="major" id="submission">
								<h2>Submission Information</h2>
							</header>

							<p>
							Submission URL:  <a href="https://cmt3.research.microsoft.com/PPAI2024">https://cmt3.research.microsoft.com/PPAI2024</a>
							</p>

							<h3>Submission Types</h3>
							<ul>
								<li><strong>Technical Papers</strong>:
								Full-length research papers of up to 7 pages (excluding references and appendices)
								detailing high quality work in progress or work that could potentially be published at
								a major conference.
								</li>
								<li><strong>Short Papers</strong>:
								Position or short papers of up to 4 pages (excluding references and appendices) that
								describe initial work or the release of privacy-preserving benchmarks and datasets on the
								topics of interest.
								</li>
							</ul>

							<h4>NeurIPS/AAAI Fast Track (Rejected AAAI papers)</h4>
							<p>
							Rejected NeurIPS/AAAI papers with *average* scores of <strong>at least 4.0</strong>
							may be submitted directly to PPAI
							along with previous reviews. These submissions may go through a light review process or
							accepted if the provided reviews are judged to meet the workshop standard.
							</p>

							<p>
							All papers must be submitted in PDF format, using the <a href="https://aaai.org/authorkit24-2/">AAAI-24 author kit</a>.
							Submissions should include the name(s), affiliations, and email addresses of all authors.
							<br>
							Submissions will be refereed on the basis of technical quality, novelty, significance, and
							clarity. Each submission will be thoroughly reviewed by at least two program committee members.
							</p>
							<p>
							NeurIPS/AAAI fast track papers are subject to the same page limits of standard submissions.
							Fast track papers should be accompanied by their reviews, submitted as a supplemental material.
							<!-- Papers will be selected for oral and/or poster presentation at the workshop.  -->
							</p>

							<p>
							For questions about the submission process, contact the workshop <a href="#contact">chairs</a>.
							</p>

						<br>

             			<header class="major" id="scholarship">
							<h2>PPAI-24 scholarship application</h2>
						    </header>

						<p>PPAI is pleased to announce a Student scholarship program for 2024. The program provides partial
							travel support for students who are full-time undergraduate or graduate students at colleges and universities;
							have submitted papers to the workshop program or letters of recommendation from their faculty advisor.
							<br><br>
							Preference will be given to participating students presenting papers at the workshop or to students from underrepresented countries and communities.
						</p>
							<p>To participate please fill in the <a href="https://forms.gle/GqX6PJ3bo5LHJrtn7" target="blank">Student Scholarship Program application form</a>.</p>
							<p>Deadline: January, 10, 2024.</p>
						    <br>

	                       <header class="major" id="registration">
							<h2>Registration</h2>
						    </header>
						    Registration in each workshop is required by all active participants, and is also open to all interested individuals.
						    <strong>Early registration deadline is on January 6th</strong>. For more information please refer to
						    <a href="https://aaai.org/aaai-conference/aaai-24-workshop-program/">AAAI-24 Workshop page</a>.
						</section>

						<section>
							<header class="major" id="program">
								<h2>Program</h2>
							</header>
							<strong>February, 26, 2024</strong><br>
							All times are in Pacific Standard Time (UTC-8).</br><br>


							<div class="table-wrapper">
								<table>
									<thead>
										<tr>
											<th colspan="1">Time</th>
											<th>Talk / Presenter </th>
										</tr>
									</thead>
									<tbody>
										<tr><td>8:50</td>	<td>Introductory remarks</td></tr>
										<tr><td>9:00</td>	<td>Invited Talk: <a href="#matt">Everything Looks Like a Nail</a>  by <em>Matthew Jagielski</em></td> </tr>
										<!-- <tr height="8px"> -->
										<!-- <td colspan="3">Session chair: <em></em></td></tr> -->
										<tr><td>9:30</td>	<td><a href="#posters">Poster Session</td></tr>
										<tr><td>10:30</td>      <td colspan="1">Break</td></tr>

										<!-- <td colspan="3">Session chair: <em></em></td></tr> -->
										<tr><td>11:00</td>	<td>Tutorial: <a href="tutorial">Why do we care about privacy?</a> by Katherine Lee</td></tr>
										<tr><td>11:45</td>	<td><a href="#roundtables">Roundtable discussions</a></td></tr>
										<tr><td>12:30</td>  <td colspan="1">Lunch Break</td></tr>

										<!-- <tr><td>14:00</td>	<td>Flash Poster Presentations</td></tr> -->
										<tr><td>14:00</td>	<td>Invited Talk: <a href="#peter">Navigating Privacy Risks in (Large) Language Models: Strategies and Solutions</a> by <em>Peter Kairouz</em></td> </tr>
										<tr><td>14:30</td>	<td><a href="#sp1">Spotlight Talk</a>: Text Sanitization Beyond Specific Domains: Zero-Shot Redaction & Substitution with Large Language Models</td></tr>
										<tr><td>14:40</td>	<td><a href="#sp2">Spotlight Talk</a>: Epsilon*: Privacy Metric for Machine Learning Models</td></tr>
										<tr><td>14:50</td>	<td><a href="#sp3">Spotlight Talk</a>: De-amplifying Bias from Differential Privacy in Language Model Fine-tuning</td></tr>
										<tr><td>15:00</td>	<td>Invited Talk: <a href="#gabriela">Unregulated? Think Again:  Unpacking all the meaningful ways in which data privacy law regulates Generative AI</a> by <em>Gabriela Zanfir-Fortunagab</em></td> </tr>
										<tr><td>15:30</td>  <td colspan="2">Break</td></tr>

										<tr><td>16:00</td>	<td>Invited Talk: <a href="#hima"> Enforcing Right to Explanation: Algorithmic Challenges and Opportunities</a> by <em>Himabindu Lakkaraju</em></td> </tr>
										<tr><td>16:30</td>  <td><a href="#panel">Panel Discussion</a>: Privacy in Generative AI - a technical and policy discussion. Where are we and what are we missing?</td></tr>
										<tr><td>17:15</td>  <td colspan="1">Concluding Remarks</td></tr>
									</tbody>
								</table>
							</div>
					</section>

					<section>
							<header class="major" id="accepted_papers">
								<h3>Accepted Papers</h3>
							</header>

								<h5 id="spotlight">Spotlight Presentations</h5>
								<ul>

									<li><font color="#f56a6a">Text Sanitization Beyond Specific Domains: Zero-Shot Redaction & Substitution with Large Language Models</font><br>
										Federico Albanese Federico Albanese (University of Buenos Aires)*; Daniel Ciolek (National University of Quilmes); Nicolas D'Ippolito (ASAPP)
									</li>

									<li><font color="#f56a6a">Epsilon*: Privacy Metric for Machine Learning Models</font><br>
										Diana Negoescu Diana Negoescu (LinkedIn Corporation)*; Humberto Gonzalez (LinkedIn Corporation); Saad Eddin Al Orjany (LinkedIn); Jilei Yang (LinkedIn Corporation); Yuliia Lut (LinkedIn Corporation); Rahul Tandra (LinkedIn Corporation); Xiaowen Zhang (LinkedIn Corporation); Xinyi Zheng (University of Michigan, Ann Arbor, Michigan); Zach Douglas (LinkedIn Corporation); Vidita Nolkha (LinkedIn Corporation); Parvez Ahammad (LinkedIn); Gennady Samorodnitsky (Cornell University)
									</li>

									<li><font color="#f56a6a">De-amplifying Bias from Differential Privacy in Language Model Fine-tuning</font><br>
										Sanjari Srivastava Sanjari Srivastava (Stanford University)*; Piotr Mardziel (Independent); Zhikun Zhang (CISPA Helmholtz Center for Information Security); Archana Ahlawat (Princeton University); anupam datta (Carnegie Mellon University); John Mitchell (Stanford University)
									</li>

								</ul>

								<h5>Poster Presentations</h5>
								<ul>

									<li><font color="#f56a6a">Towards Machine Unlearning Benchmarks: Forgetting the Personal Identities in Facial Recognition Systems</font><br>
										Dongbin Na Dasol Choi (Kyunghee University); Dongbin Na (POSTECH)*
										</li>

										<li><font color="#f56a6a">Randomized algorithms for precise measurement of differentially-private, personalized recommendations</font><br>
										Allegra Laro Allegra Laro (Apple)*; Yanqing Chen (Apple); Hao He (Apple); Babak Aghazadeh (Apple)
										</li>

										<li><font color="#f56a6a">Differentially Private and Adversarially Robust Machine Learning: An Empirical Evaluation</font><br>
										Janvi Thakkar Janvi Thakkar (Imperial College London)*; Giulio Zizzo (IBM Research); Sergio Maffeis (Imperial College London)
										</li>

										<li><font color="#f56a6a">Differentially Private Neural Tangent Kernels for Privacy-Preserving Data Generation</font><br>
										Yilin Yang Yilin Yang (University of British Columbia)*; Mi Jung Park (UBC )
										</li>

										<li><font color="#f56a6a">RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization</font><br>
										Ce Feng Ce Feng (Lehigh University)*; parv Venkitasubramaniam (Lehigh University)
										</li>

										<li><font color="#f56a6a">Evaluating Privacy Leakage in Split Learning</font><br>
										Xinchi Qiu Xinchi Qiu (University of Cambridge)*; Ilias Leontiadis (Samsung Ai); Luca Melis (Meta); Alexandre Sablayrolles (Facebook AI Research); Pierre Stock ()
										</li>

										<li><font color="#f56a6a">Don't Forget Private Retrieval: Distributed Private Similarity Search for Large Language Models</font><br>
										Tobin South Guy Zyskind (MIT); Tobin South (MIT)*; Alex `Sandy' Pentland (MIT)
										</li>

										<li><font color="#f56a6a">Empirical Privacy Trade-Off Curves: Understanding the Gap between Theoretical and Practical Privacy Guarantees</font><br>
										Mohammad Yaghini Mohammad Yaghini (University of Toronto & Vector Institute)*; Lukas Wutschitz (Microsoft); Santiago Zanella-Beguelin (Microsoft Research)
										</li>

										<li><font color="#f56a6a">Why Does Differential Privacy with Large $\varepsilon$ Defend Against Practical Membership Inference Attacks?</font><br>
										Andrew Lowy Andrew Lowy (USC)*; Zhuohang Li (Vanderbilt University); Jing Liu (MERL); Toshiaki Koike-Akino (Mitsubishi Electric Research Laboratories); Kieran Parsons (Mitsubishi Electric Research Laboratories); Ye Wang (Mitsubishi Electric Research Laboratories)
										</li>

										<li><font color="#f56a6a">Group Decision-Making among Privacy-Aware Agents</font><br>
										Marios Papachristou Marios Papachristou (Cornell)*; Amin Rahimian (University of Pittsburgh)
										</li>

										<li><font color="#f56a6a">Differentially Private Training of Mixture of Experts Models</font><br>
										Pierre Tholoniat Pierre Tholoniat (Columbia University)*; Huseyin Inan (Microsoft Research ); Janardhan Kulkarni (Microsoft Research); Robert Sim (Microsoft Research)
										</li>

										<li><font color="#f56a6a">An Efficient and Accurate Gated RNN for Execution under TFHE</font><br>
										Rickard Brännvall Rickard Brännvall (RISE Research Institutes of Sweden)*; Andrei Stoian (ZAMA)
										</li>

										<li><font color="#f56a6a">How To Filter Out Malicious Encrypted Gradients in Federated Learning</font><br>
										Sílvia Casacuberta Jordan Barkin (Harvard University); Ratip Emin Berker (Harvard University); Sílvia Casacuberta (Harvard University)*; Janet Li (Harvard University)
										</li>

										<li><font color="#f56a6a">Qrlew: Query Rewriting for Differential Privacy</font><br>
										Nicolas Grislain Nicolas Grislain (Sarus)*; Paul Roussel (Sarus); Victoria de Sainte Agathe (Sarus)
										</li>

										<li><font color="#f56a6a">Applying Directional Noise to Deep Learning</font><br>
										Pedro Faustini Pedro Faustini (Macquarie University)*; Natasha Fernandes (Macquarie University); Shakila Tonni (Macquarie University); Annabelle McIver (Macquarie University); Mark Dras (Computing Department, Macquarie University, NSW 2109)
										</li>

										<li><font color="#f56a6a">Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated Learning</font><br>
										Luyao Niu Zhangchen Xu (University of Washington); Fengqing Jiang (University of Washington); Luyao Niu (University of Washington)*; Jinyuan Jia (The Pennsylvania State University); Radha Poovendran (University of Washington)
										</li>

										<li><font color="#f56a6a">Differentially Private Prediction of Large Language Models</font><br>
										James Flemings James Flemings (University of Southern California)*; Murali Annavaram (University of Southern California); Meisam Razaviyayn (University of Southern California)
										</li>

										<li><font color="#f56a6a">Tuning Differential Privacy Mechanisms Using Causal Models of Contextual Integrity</font><br>
										Sebastian Benthall Sebastian Benthall (International Computer Science Institute)*; Rachel Cummings (Columbia University)
										</li>

										<li><font color="#f56a6a">Training Differentially Private Ad Prediction Models with Semi-Sensitive Features</font><br>
										Badih Ghazi Badih Ghazi (Google)*; Amer Sinha (Google); Avinash Varadarajan (Google AI Healthcare); Chiyuan Zhang (Google); Lynn Chua (Google); Charlie Harrison (Google); Qiliang Cui (Google); Krishna Giri Narra (University of Southern California); Pasin Manurangsi (Google); Pritish Kamath (Google Research); Walid Krichene (Google); Ravi Kumar (Google)
										</li>

										<li><font color="#f56a6a">Benchmarking Private Population Data Release Mechanisms: Synthetic Data vs. TopDown</font><br>
										Aadyaa Maddi Aadyaa Maddi (Carnegie Mellon University)*; Swadhin Routray (Carnegie Mellon University); Alexander Goldberg (Carnegie Mellon University); Giulia Fanti (CMU)
										</li>

										<li><font color="#f56a6a">Prεεmpt: Sanitizing Sensitive Prompts for LLMs</font><br>
										Amrita Roy Chowdhury Amrita Roy Chowdhury (UCSD)*; David Glukhov (University of Toronto); Divyam Anshumaan (University of Wisconsin-Madison); Prasad CHALASANI (XaiPient); Nicolas Papernot (University of Toronto and Vector Institute); Somesh Jha (University of Wisconsin-Madison and XaiPient)
										</li>

										<li><font color="#f56a6a">Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication</font><br>
										Ali Naseh Ali Naseh (University of Massachusetts Amherst)*; Jaechul Roh (University of Massachusetts Amherst ); Amir Houmansadr (University of Massachusetts Amherst)
										</li>

										<li><font color="#f56a6a">High Epsilon Synthetic Data Vulnerabilities in MST and PrivBayes</font><br>
										Steven Golob Steven Golob (University of Washington Tacoma)*; Sikha Pentyala (University of Washington, Tacoma); Anuar Maratkhan (University of Washington Tacoma); Martine De Cock (University of Washington Tacoma)
										</li>

										<li><font color="#f56a6a">I Can’t See It But I Can Fine-tune It: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption</font><br>
										Prajwal Panzade Prajwal Panzade (Georgia State University)*; Daniel Takabi (Old Dominion University); Zhipeng Cai (Georgia State University)
										</li>

										<li><font color="#f56a6a">Performance Fairness in Differentially Private Federated Learning</font><br>
										Saber Malekmohammadi Saber Malekmohammadi (University of Waterloo)*; Afaf Taik (Mila - Quebec AI Institute, Université de Montréal); Golnoosh Farnadi (McGill University, Mila, Université de Montréal, Google)
										</li>

										<li><font color="#f56a6a">Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in Generative AI Interactions</font><br>
											Bianca-Mihaela Ganescu (Imperial College London); Jonathan Passerat-Palmbach (Imperial College London / Flashbots)
										</li>

										<li><font color="#f56a6a">Achieving Certified Fairness with Differential Privacy</font><br>
											Hai Phan Khang Tran (New Jersey Institute of Technology); Ferdinando Fioretto (University of Virginia); Issa Khalil (Qatar Computing Research Institute); My T. Thai (University of Florida); Hai Phan (New Jersey Institute of Technology)
										</li>

										<li><font color="#f56a6a">DPFedSub: A Uncompromisingly Differentially Private Federated Learning Scheme with Randomized Subspace Descent Optimization</font><br>
											Chuan Ma Huiwen Wu (Zhejiang Lab); Cen Chen (East China Normal University); Chuan Ma (Zhejiang Lab)*; TianFang Wang (ZHEJIANG LAB); Zhe Liu (Zhejiang Lab)
										</li>
								</ul>
					</section>

					<section>
								<header class="major" id="tutorials">
									<h3>Tutorial</h3>
								</header>
								<h4 id="tutorial">
									<strong>Why do we care about privacy?</strong>
										<a href="files/Katerine_talk.pdf" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a>
										<!-- <a href="#" target="_blank"><img src="images/play.png" width="20" style="vertical-align:middle" alt="video recording"></a> -->
								</h4>
								by <a href="https://katelee168.github.io">Katherine Lee</a> (Google DeepMind).
								<p>
								<strong>Abstract</strong>:
								“Privacy” means something different to every person. And “privacy” means something different in different jurisdictions. Together, we’ll explore different notions of privacy, and impacts of different privacy legislation. We’ll discuss some policies set by governments and products, and brainstorm how we might meet those notions of privacy.
								<br>
								</p>

								<header class="major" id="invited_talks">
									<h3>Invited Talks</h3>
								</header>

								<h4 id="matt">
									<strong>Everything Looks Like a Nail </strong>
									<a href="files/Matthew_talk.pdf" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a>
								</h4>
								by <a href="https://jagielski.github.io">Matthew Jagielski</a> (Google DeepMind)
								<p>
									<strong>Abstract</strong>: <br>
									TGood definitions are one of the greatest achievements of privacy research; in machine learning, frameworks ("hammers") such as differential privacy and machine unlearning have spurred successful research programs and practical applications. In the wake of the recent explosion in interest in large, general purpose models, it is natural to begin applying our existing tools to these more modern applications. In this talk, I will discuss scenarios that make our existing hammers break down, which I hope will motivate creative future research.
								</p>
								<!-- <p>
									<strong>Bio</strong>: <br>
								</p> -->

								<h4 id="peter">
									<strong>Navigating Privacy Risks in (Large) Language Models: Strategies and Solutions</strong>
									<a href="files/Peter_talk.pdf" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a>
								</h4>
								by <a href="https://kairouzp.github.io/">Peter Kairouz</a> (Google)
								<p>
									<strong>Abstract</strong>: <br>
									The new wave of large language models (LLMs) is spurring a suite of exciting opportunities: from content generation to question answering and information retrieval. However, the process of training, fine-tuning, and deploying these models comes with several important risks, including privacy, the focus of this talk.
									<br>
									Building on a comprehensive taxonomy for privacy threats, we demonstrate privacy vulnerabilities in LLMs fine-tuned on user data. We then show how these risks can be mitigated using a combination of techniques such as federated learning and user-level differential privacy, albeit with increased computational demands. We finally demonstrate how even moderate user-level differential privacy can completely mitigate risks against many realistic threat models. We do so by presenting a novel method that is capable of accurately estimating the privacy leakage in a one-shot fashion (i.e. in a single training run).
								</p>
								<!-- <p>
									<strong>Bio</strong>: <br>
									Peter Kairouz is a Research Scientist at Google, where he focuses on researching and building private, secure, and trustworthy AI technologies. Before joining Google, he was a Postdoctoral Research Fellow at Stanford University. He received his Ph.D. in electrical and computer engineering from the University of Illinois at Urbana-Champaign (UIUC). He is the recipient of the 2012 Roberto Padovani Scholarship from Qualcomm's Research Center, the 2015 ACM SIGMETRICS Best Paper Award, the 2015 Qualcomm Innovation Fellowship Finalist Award, the 2016 Harold L. Olesen Award for Excellence in Undergraduate Teaching from UIUC, and the 2021 ACM Conference on Computer and Communications Security (CCS) Best Paper Award.
								</p> -->


								<h4 id="gabriela">
									<strong>Unregulated? Think Again:  Unpacking all the meaningful ways in which data privacy law regulates Generative AI</strong>
									<a href="files/Gabriela_talk.pptx" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a>
								</h4>
								by <a href="https://pdpecho.com/about">Gabriela Zanfir-Fortunagab</a> (Future of Privacy Forum)
								<p>
									<strong>Abstract</strong>: <br>
									The Italian Privacy Commission banned ChatGPT in the country for a month in March 2023 due to several alleged breaches of the General Data Protection Regulation (GDPR). This ban rang the alarm just as Generative AI was experiencing unprecedented growth in the realm of consumer applications. Soon after, Data Protection Authorities and Privacy Commissioners in Canada, South Korea, Japan, Brazil, Poland and the US announced they started investigations into ChatGPT in the application of their privacy and data protection legal frameworks. In October last year, the Global Privacy Assembly, an international organization reuniting more than 130 privacy and data protection commissioners from around the world, adopted a “Resolution on Generative Artificial Intelligence Systems”. It lays out how the decades old fair information practice principles, such as data minimization and purpose limitation, which are encoded in data privacy laws must be observed by developers and deployers of Generative AI systems. All of this is happening under the radar, as the public is paying attention to prominent Summits on AI and other intergovernmental initiatives that debate under the spotlight best choices for vague future AI governance frameworks. This talk will explore all the meaningful and extremely concrete ways in which data protection law is applicable, now, to Generative AI in particular, and AI systems more generally. It will also explain why exactly data privacy is so relevant to how these technologies interact with the data of their users, or the data in their training datasets.

								</p>
								<!-- <p>
									<strong>Bio</strong>: <br>
								</p> -->

								<h4 id="hima">
									<strong> Enforcing Right to Explanation: Algorithmic Challenges and Opportunities</strong>
									<a href="files/Hima_talk.pptx" target="_blank"><img src="images/pdf-icon.png" width="20" style="vertical-align:middle" alt="slides"></a>
								</h4>
								by <a href="https://pdpecho.com/about">Himabindu Lakkaraju</a> (Harvard University)
								<p>
									<strong>Abstract</strong>: <br>
									As predictive and generative models are increasingly being deployed in various high-stakes applications in critical domains including healthcare, law, policy and finance, it becomes important to ensure that relevant stakeholders understand the behaviors and outputs of these models so that they can determine if and when to intervene. To this end, several techniques have been proposed in recent literature to explain these models. In addition, multiple regulatory frameworks (e.g., GDPR, CCPA) introduced in recent years also emphasized the importance of enforcing the key principle of “Right to Explanation” to ensure that individuals who are adversely impacted by algorithmic outcomes are provided with an actionable explanation. In this talk, I will discuss the gaps that exist between regulations and state-of-the-art technical solutions when it comes to explainability of predictive and generative models. I will then present some of our latest research that attempts to address some of these gaps. I will conclude the talk by discussing bigger challenges that arise as we think about enforcing right to explanation in the context of large language models and other large generative models.
								</p>
								<!-- <p>
									<strong>Bio</strong>: <br>
								</p> -->

								<header class="major" id="roundtables">
									<h3>Roundtable Discusssions</h3>
								</header>
									Roundtable discussions are informal conversations in which ALL workshop audience is asked to actively participate regarding topics of interest, fostering a collaborative environment for exchanging ideas, experiences, and insights.

								<h4 id="r1">
									<strong>Privacy in Generative AI</strong>
								</h4>
								moderated by <a href="">TBA</a>.
								<p>

									<h4 id="r1">
										<strong>Privacy and Policy</strong>
									</h4>
									moderated by <a href="https://gfarnadi.github.io">Golnoosh Farnadi (McGill University)</a>.
									<p>
						</section>



						<section>
							<header class="major" id="invited">
								<h2>Invited Speakers</h2>
							</header>

							<div class="row">
								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://katelee168.github.io"><img class="center" src="images/katherine_lee.jpeg" alt=""/></a></span>
									<div class="content"><p class="name">Katherine Lee</p>Google DeepMind<br><br>
										<p><a href="#katherine">Tutorial details</a></p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://kairouzp.github.io/"><img class="center" src="images/peter.jpg" alt=""/></a></span>
									<div class="content"><p class="name">Peter Kairouz</p>Google<br><br>
										<p><a href="#peter">Talk details</a></p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://jagielski.github.io"><img class="center" src="images/matthew_jagielski.jpeg" alt=""/></a></span>
									<div class="content"><p class="name">Matthew Jagielski</p>Google DeepMind<br><br>
										<p><a href="#matt">Talk details</a></p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://pdpecho.com/about/"><img class="center" src="images/GabrielaFortuna.jpeg" alt=""/></a></span>
									<div class="content"><p class="name">Gabriela Zanfir-Fortunagab</p>Future of Privacy Forum<br><br>
										<p><a href="#gabriela">Talk details</a></p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://pdpecho.com/about/"><img class="center" src="https://www.hbs.edu/Style%20Library/api/headshot.aspx?id=1057381" alt=""/></a></span>
									<div class="content"><p class="name">Himabindu Lakkaraju</p>Harvard University<br><br>
										<p><a href="#hima">Talk details</a></p>
									</div>
								</div>
							</div>


							<header class="major" id="panel">
								<h2>PPAI-24 Panel:</h2>
								<h3>Privacy in Generative AI - a technical and policy discussion. Where are we and what are we missing?</h3>
							</header>

							<h3>Confirmed Panelists</h3>

							<div class="row">
								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://www.nist.gov/people/gary-howarth"><img class="center"
										src="https://www.nist.gov/sites/default/files/styles/480_x_480_limit/public/images/2021/01/27/Gary%20Howarth%20Headshot.jpg?itok=jE-a-oBc" alt=""/></a></span>
									<div class="content"><p class="name">Gary S. Howarth</p>Physical Scientist, NIST<br><br></div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://katelee168.github.io"><img class="center" src="images/katherine_lee.jpeg" alt=""/></a></span>
									<div class="content"><p class="name">Katherine Lee</p>Google DeepMind<br><br> <p><a href="#katherine">Tutorial details</a></p></div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://www.linkedin.com/in/ayaz-minhas-3783b917/"><img class="center"
										src="https://media.licdn.com/dms/image/C4E03AQGOjWqzFBZLuw/profile-displayphoto-shrink_400_400/0/1586885145232?e=1710374400&v=beta&t=o6I3Gj7DDJCgMawzQ2foSr2Eb-I0DWviepxCBc5OtNk" alt=""/></a></span>
									<div class="content"><p class="name">Ayaz Minhas</p>Privacy Policy Manager, Artificial Intelligence at Meta<br><br></div>
								</div>

							<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://sethneel.com"><img class="center" src="https://sethstatistics.files.wordpress.com/2022/06/cropped-headshot.jpg" alt=""/></a></span>
									<div class="content"><p class="name">Seth Neel</p>Assistant Professor, Harvard<br><br></div>
							</div>

							<div class="col-4 col-12-medium">
								<span class="image left"><a href=""><img class="center" src="https://partnershiponai.org/wp-content/uploads/2021/07/ParkTinaM_Headshot.png" alt=""/></a></span>
								<div class="content"><p class="name">Tina M. Park</p>Head of Inclusive Research and Design at Partenrship for AI<br><br></div>
							</div>

						</div>
						<br>
						</section>

						<section>
							<header class="major" id="sponsors">
								<h2>Sponsors</h2>
							</header>
							<h3>Diamond</h3>
							<div class="row">
								<div class="col-4 col-12-medium">
								<img class="center" src="images/google-logo.png "alt="Google" height="80"><br>
							</div>
							</div>
							<br>

							<h3>Gold</h3>
							<div class="row">
								<div class="col-4 col-12-medium">
								<img class="center" src="images/GeorgiaTech.png "alt="Georgia Tech" height="80"><br>
								</div>
							</div>

							<h3>Silver</h3>
							<div class="row">
								<div class="col-4 col-12-medium">
								<img class="center" src="images/logo-opendp.png" alt="Open DP" height="80"><br>
								</div>
								<div class="col-4 col-12-medium">
								<img class="center" src="images/njit_logo.png" alt="NJIT" height="80"><br>
								</div>
							</div>

							<br><br><br>
							<p>
							Interested in being a PPAI sponsor? Take a look at our <a href="files/PPAI 2024 Sponsorship.pdf">Sponsors Tiers</a>.
							</p>


						<header class="major" id="code">
							<h2>Code of Conduct</h2>
							</header>
							PPAI 2024 is committed to providing an atmosphere that encourages freedom of expression and exchange of ideas. It is the policy of PPAI 2024 that all participants will enjoy a welcoming environment free from unlawful discrimination, harassment, and retaliation.
							<br><br>
							Harassment will not be tolerated in any form, including but not limited to harassment based on gender, gender identity and expression, sexual orientation, disability, physical appearance, race, age, religion or any other status. Harassment includes the use of abusive, offensive or degrading language, intimidation, stalking, harassing photography or recording, inappropriate physical contact, sexual imagery and unwelcome sexual advances. Participants asked to stop any harassing behavior are expected to comply immediately.
							<br><br>
							Violations should be reported to the <a href="#contact">workshop chairs</a>. All reports will be treated confidentially. The conference committee will deal with each case separately. Sanctions include, but are not limited to, exclusion from the workshop, removal of material from the online record of the conference, and  referral to the violator’s university or employer.
							All PPAI 2024 attendees are expected to comply with these standards of behavior.
						<br><br><br>
					</section>

						<section>
							<header class="major" id="pc">
								<h2>Program Committee</h2>
							</header>
							<ul>
			<li>Abdullatif Mohammed	Albaseer 	- HBKU</li>
			<li>Ajinkya	Mulay					- Purdue University</li>
			<li>Ali	Ghafelebashi				- University of Southern California	</li>
			<li>Amin	Rahimian				- University of Pittsburgh</li>
			<li>Antti	Koskela					- Nokia Bell Labs</li>
			<li>Audra	McMillan				- Apple</li>
			<li>Aurélien	Bellet				- INRIA</li>
			<li>Catuscia	Palamidessi			- Laboratoire d'informatique de l'École polytechnique	</li>
			<li>Clément	Canonne	clement.		- University of Sydney</li>
			<li>Difang	Huang					- University of Hong Kong</li>
			<li>Diptangshu	Sen					- Georgia Institute of Technology</li>
			<li>Elette	Boyle					- IDC Herzliya</li>
			<li>Ero	Balsa	ero.				- Cornell Tech</li>
			<li>Fan	Mo							- Imperial College London</li>
			<li>Gautam	Kamath					- University of Waterloo</li>
			<li>Gharib	Gharibi				 	- TripleBlind</li>
			<li>Graham	Cormode	g.				- University of Warwick</li>
			<li>Hongyan	Chang					- National University of Singapore</li>
			<li>Ivoline	Ngong					- Konya Technical University	</li>
			<li>James	Flemings				- University of Southern California</li>
			<li>Jason	Mancuso					- Cape Privacy</li>
			<li>Jie	Huang						- University of Illinois at Urbana-Champaign</li>
			<li>Karthick Prasad	Gunasekaran		- Amazon</li>
			<li>Keegan	Harris					- Carnegie Mellon University	</li>
			<li>Krishna	Acharya					- Georgia Institute of Technology</li>
			<li>Krishna Sri Ipsit	Mantri		- Purdue University</li>
			<li>Krystal	Maughan	krystal.		- University of Vermont</li>
			<li>Lucas	Rosenblatt	lucas.		- New York University</li>
			<li>Ludmila	Glinskih				- Boston University</li>
			<li>Luyao	Zhang					- Duke Kunshan University</li>
			<li>Marco	Romanelli				- New York University</li>
			<li>Mohammad	Naseri				- University College London</li>
			<li>Moshe	Shenfeld				- Hebrew University	</li>
			<li>Muhammad Habib	 ur Rehman		- King's College London</li>
			<li>Pierre	Tholoniat				- Columbia University</li>
			<li>Pratiksha	Thaker				- Carnegie Mellon University</li>
			<li>Rakshit	Naidu					- Georgia Institute of Technology</li>
			<li>Ranya	Aloufi				 	- Imperial College</li>
			<li>Robert	Mahari					- Massachusetts Institute of Technology</li>
			<li>Rojin	Rezvan 					- University of Wisconsin-Madison</li>
			<li>Roozbeh	Yousefzadeh	roozbeh.	- Lightmatter</li>
			<li>Rui-Jie	Yew						- Brown University</li>
			<li>Sahib	Singh					- Ford Research (R&A)</li>
			<li>Sankarshan	Damle	 			- International Institute of Information Technology, Hyderabad	</li>
			<li>Saswat	Das						- University of Virginia</li>
			<li>Seohui	Bae						- LG AI Research</li>
			<li>Sina	Sajadmanesh	sina.		- Sony AI</li>
			<li>Stacey	Truex					- Denison University</li>
			<li>Stephen	Casper					- Massachusetts Institute of Technology</li>
			<li>Tao	Lin							- Harvard University</li>
			<li>Tianhao	Wang					- University of Virginia</li>
			<li>Vahid	Behzadan				- University of New Haven</li>
			<li>Vandy	Tombs					- Oak Ridge National Laboratory</li>
			<li>Vasanta	Chaganti				- Swarthmore College</li>
			<li>Vincent Tao	Hu					- University of Amsterdam	</li>
			<li>Wanrong	Zhang					- Harvard University</li>
			<li>Xi	He							- University of Waterloo</li>
			<li>Yeojoon	Youn					- Georgia Institute of Technology</li>
			<li>Yijun	Bian					- University of Science and Technology of China</li>
			<li>Yongjun	Zhao					- TikTok	</li>
			<li>Yulu	Jin						- University of California, Davis</li>
			</ul>

 							<br>
							<header class="major" id="chairs">
								<h2>Workshop Chairs</h2>
							</header>

							<div class="row">
								<!-- <article> -->
								<div class="col-4 col-12-medium">
									<span class="image left">
										<a href="https://www.nandofioretto.com"><img class="center" src="images/Nando.png" alt=""/></a>
									</span>
									<div class="content">
										<p class="name">Ferdinando Fioretto</p>
										University of Virginia
										<br><br>
										<p>
											<a href="mailto:fioretto@virginia.edu">fioretto@virginia.edu</a>
										</p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://cseweb.ucsd.edu/~fmireshg/"><img class="center" src="images/Nilofar.png" alt=""/></a></span>
									<div class="content">
										<p class="name">Niloofar Mireshghallah</p>
										University of Washington
										<br><br>
										<p>
											<a href="mailto:niloofar@cs.washington.edu">niloofar@cs.washington.edu</a>
										</p>
									</div>
								</div>

								<div class="col-4 col-12-medium">
									<span class="image left"><a href=""><img class="center" src="images/Christine_Task.jpg" alt=""/></a></span>
									<div class="content">
										<p class="name">Christine Task</p>
										Knexus Research Corporation
										<br><br>
										<p>
											<a href="mailto:christine.task@knexusresearch.com">christine.task@knexusresearch.com</a>
										</p>
									</div>
								</div>

								<!-- <article> -->
								<div class="col-4 col-12-medium">
									<span class="image left"><a href="https://sites.gatech.edu/pascal-van-hentenryck/"><img class="center" src="images/Pascal.png" alt=""/></a></span>
									<div class="content">
										<p class="name">Pascal Van Hentenryck</p>
										Georgia Institute of Technology
										<br><br>
										<p>
											<a href="mailto:pvh@isye.gatech.edu">pvh@isye.gatech.edu</a>
										</p>
									</div>
								</div>

								<!-- <article> -->
								<div class="col-4 col-12-medium">
									<span class="image left"><a href="http://www.juba-ziani.com">
										<img class="center" src="images/Juba.jpg" alt=""/></a></span>
									<div class="content">
										<p class="name">Juba Ziani</p>
										Georgia Institute of Technology										<br><br>
										<p>
											<a href="mailto:juba.ziani@isye.gatech.edu">juba.ziani@isye.gatech.edu</a>
										</p>
									</div>
								</div>
							</div>
						</section>
				</div>
			</div>

		<!-- Sidebar -->
			<div id="sidebar">
				<div class="inner">
					<!-- Menu -->
						<nav id="menu">
							<header class="major">
								<h2>PPAI24</h2>
							</header>
							<ul>
								<li><a href="#scope">Scope</a></li>
								<li><a href="#dates">Important Dates</a></li>
								<li><a href="#submission">Submission</a></li>
								<li>
									<span class="opener">Program</span>
									<ul>
									<li><a href="#program">Schedule</a></li>
									<li><a href="#tutorials">Tutorials</a></li>
									<li><a href="#invited_talks">Invited Talks</a></li>
									<li><a href="#panel">Panel</a></li>
									</ul>
								</li>
                                <li><a href="#scholarship">Scholarships</a></li>
                                <li><a href="#registration">Registration</a></li>
								<li><a href="#accepted_papers">Accepted Papers</a></li>
								<li><a href="#invited">Invited Speakers</a></li>
 								<li><a href="#pc">Program Committee</a></li>
								<li><a href="#chairs">Workshop Chairs</a></li>
								<li>
									<span class="opener">Previous Editions</span>
									<ul>
									<li><a href="https://aaai-ppai23.github.io/">PPAI 2023</a></li>
									<li><a href="https://aaai-ppai22.github.io/">PPAI 2022</a></li>
									<li><a href="https://ppai21.github.io/">PPAI 2021</a></li>
									<li><a href="https://www2.isye.gatech.edu/~fferdinando3/cfp/PPAI20/">PPAI 2020</a></li>
									</ul>
								</li>
							</ul>
						</nav>

					<!-- Section -->
						<section>
							<header class="major">
								<h2>Contacts</h2>
							</header>
<!-- 									<p>.</p> -->
						<ul class="contact">
							<li class="icon solid fa-envelope"><a href="mailto:ppaiworkshop@gmail.com">Email chairs</a></li>
							<!-- <li class="icon brands fa-twitter"><a href="https://twitter.com/PPAI211">Twitter</a></li> -->
						</ul>
						</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">Website template from: <a href="https://html5up.net">HTML5 UP</a>.</p>
							</footer>

					</div>
				</div>

		</div>

	<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
	</body>
</html>
